#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SCPIæŒ‡ä»¤æ™ºèƒ½æœç´¢å·¥å…· - æœ€ç»ˆä¼˜åŒ–ç‰ˆæœ¬
ä¸“é—¨é’ˆå¯¹å½’ä¸€åŒ–å‘é‡ä¼˜åŒ–çš„æœç´¢ç®—æ³•
ç¡®ä¿å‘é‡åŒ¹é…ç²¾ç¡®æ€§å’Œæœç´¢å‡†ç¡®æ€§
"""

import json
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModel
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import normalize
import argparse
import sys
from typing import List, Dict, Tuple
import warnings

warnings.filterwarnings("ignore")

class SCPICommandSearcherFinal:
    def __init__(self, json_file_path: str, model_name: str = "sentence-transformers/all-MiniLM-L6-v2"):
        """
        åˆå§‹åŒ–æœ€ç»ˆä¼˜åŒ–ç‰ˆSCPIæŒ‡ä»¤æœç´¢å™¨
        
        Args:
            json_file_path: SCPIæŒ‡ä»¤JSONæ–‡ä»¶è·¯å¾„
            model_name: ä½¿ç”¨çš„transformeræ¨¡å‹åç§°
        """
        self.json_file_path = json_file_path
        self.model_name = model_name
        self.commands_data = []
        self.command_vectors = []
        
        # åˆå§‹åŒ–æ¨¡å‹
        print("æ­£åœ¨åŠ è½½transformeræ¨¡å‹...")
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = AutoModel.from_pretrained(model_name)
            print(f"æ¨¡å‹ {model_name} åŠ è½½æˆåŠŸï¼")
        except Exception as e:
            print(f"æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ¨¡å‹: {e}")
            self.model_name = "distilbert-base-uncased"
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModel.from_pretrained(self.model_name)
        
        # åŠ è½½SCPIæŒ‡ä»¤æ•°æ®
        self.load_commands_data()
        
    def load_commands_data(self):
        """åŠ è½½SCPIæŒ‡ä»¤æ•°æ®å¹¶éªŒè¯å‘é‡æ ¼å¼"""
        print("æ­£åœ¨åŠ è½½SCPIæŒ‡ä»¤æ•°æ®...")
        try:
            with open(self.json_file_path, 'r', encoding='utf-8') as f:
                self.commands_data = json.load(f)
            
            # æå–å¹¶éªŒè¯é¢„è®¡ç®—çš„å‘é‡
            vectors = []
            invalid_count = 0
            
            for i, cmd in enumerate(self.commands_data):
                if 'å‘é‡' in cmd and isinstance(cmd['å‘é‡'], list) and len(cmd['å‘é‡']) == 384:
                    vector = np.array(cmd['å‘é‡'], dtype=np.float32)
                    
                    # éªŒè¯å‘é‡æ˜¯å¦å½’ä¸€åŒ–
                    norm = np.linalg.norm(vector)
                    if abs(norm - 1.0) < 1e-6:  # å…è®¸å°çš„æ•°å€¼è¯¯å·®
                        vectors.append(vector)
                    else:
                        print(f"è­¦å‘Š: ç¬¬{i}ä¸ªå‘é‡æœªå½’ä¸€åŒ– (æ¨¡é•¿: {norm:.6f}), å°†è¿›è¡Œå½’ä¸€åŒ–")
                        vectors.append(vector / norm if norm > 1e-8 else np.zeros(384, dtype=np.float32))
                        invalid_count += 1
                else:
                    print(f"è­¦å‘Š: ç¬¬{i}ä¸ªæ¡ç›®å‘é‡æ ¼å¼å¼‚å¸¸ï¼Œä½¿ç”¨é›¶å‘é‡æ›¿ä»£")
                    vectors.append(np.zeros(384, dtype=np.float32))
                    invalid_count += 1
            
            self.command_vectors = np.array(vectors, dtype=np.float32)
            
            print(f"æˆåŠŸåŠ è½½ {len(self.commands_data)} æ¡SCPIæŒ‡ä»¤")
            print(f"å‘é‡ç»´åº¦: {self.command_vectors.shape}")
            print(f"å¼‚å¸¸å‘é‡æ•°é‡: {invalid_count}")
            
            # éªŒè¯å‘é‡å½’ä¸€åŒ–
            norms = np.linalg.norm(self.command_vectors, axis=1)
            print(f"å‘é‡æ¨¡é•¿èŒƒå›´: [{norms.min():.6f}, {norms.max():.6f}]")
            print(f"æ‰€æœ‰å‘é‡éƒ½å·²å½’ä¸€åŒ–: {np.allclose(norms, 1.0, atol=1e-6)}")
            
        except Exception as e:
            print(f"åŠ è½½æ•°æ®å¤±è´¥: {e}")
            sys.exit(1)
    
    def encode_text(self, text: str) -> np.ndarray:
        """
        å°†æ–‡æœ¬ç¼–ç ä¸ºå½’ä¸€åŒ–å‘é‡ï¼Œç¡®ä¿ä¸é¢„è®¡ç®—å‘é‡æ ¼å¼ä¸€è‡´
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            å½’ä¸€åŒ–çš„æ–‡æœ¬å‘é‡è¡¨ç¤º
        """
        # å¯¹æ–‡æœ¬è¿›è¡Œtokenization
        inputs = self.tokenizer(text, return_tensors='pt', padding=True, 
                              truncation=True, max_length=512)
        
        # è·å–æ¨¡å‹è¾“å‡º
        with torch.no_grad():
            outputs = self.model(**inputs)
            # ä½¿ç”¨å¹³å‡æ± åŒ–è·å¾—å¥å­çº§è¡¨ç¤º
            if hasattr(outputs, 'last_hidden_state'):
                embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()
            else:
                embeddings = outputs.pooler_output.squeeze().numpy()
        
        # ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´
        embeddings = embeddings.astype(np.float32)
        
        # å¤„ç†ç»´åº¦ä¸åŒ¹é…çš„æƒ…å†µ
        if embeddings.shape[0] != 384:
            if embeddings.shape[0] > 384:
                embeddings = embeddings[:384]  # æˆªæ–­
            else:
                # å¡«å……é›¶
                padded = np.zeros(384, dtype=np.float32)
                padded[:embeddings.shape[0]] = embeddings
                embeddings = padded
        
        # å½’ä¸€åŒ–å‘é‡ä»¥ä¸é¢„è®¡ç®—å‘é‡ä¿æŒä¸€è‡´
        norm = np.linalg.norm(embeddings)
        if norm > 1e-8:
            embeddings = embeddings / norm
        else:
            # å¦‚æœå‘é‡å…¨é›¶ï¼Œä½¿ç”¨éšæœºå•ä½å‘é‡
            embeddings = np.random.randn(384).astype(np.float32)
            embeddings = embeddings / np.linalg.norm(embeddings)
        
        return embeddings
    
    def calculate_similarities_optimized(self, query_vector: np.ndarray) -> Dict[str, np.ndarray]:
        """
        é’ˆå¯¹å½’ä¸€åŒ–å‘é‡ä¼˜åŒ–çš„ç›¸ä¼¼åº¦è®¡ç®—
        
        Args:
            query_vector: æŸ¥è¯¢å‘é‡ï¼ˆå·²å½’ä¸€åŒ–ï¼‰
            
        Returns:
            åŒ…å«ä¸åŒç›¸ä¼¼åº¦è®¡ç®—ç»“æœçš„å­—å…¸
        """
        similarities = {}
        
        # 1. ä½™å¼¦ç›¸ä¼¼åº¦ - å¯¹å½’ä¸€åŒ–å‘é‡æœ€é‡è¦
        cos_sim = cosine_similarity([query_vector], self.command_vectors)[0]
        similarities['cosine'] = cos_sim
        
        # 2. åŸºäºä½™å¼¦ç›¸ä¼¼åº¦çš„è§’åº¦è·ç¦»
        # å¯¹äºå½’ä¸€åŒ–å‘é‡ï¼Œè§’åº¦ = arccos(cosine_similarity)
        cos_sim_clipped = np.clip(cos_sim, -1, 1)
        angles = np.arccos(np.abs(cos_sim_clipped))  # ä½¿ç”¨ç»å¯¹å€¼è€ƒè™‘æ–¹å‘
        angle_sim = 1 - (angles / np.pi)  # å½’ä¸€åŒ–åˆ°[0,1]ï¼Œè§’åº¦è¶Šå°ç›¸ä¼¼åº¦è¶Šé«˜
        similarities['angle'] = angle_sim
        
        # 3. æ¬§æ°è·ç¦»ç›¸ä¼¼åº¦ï¼ˆå¯¹å½’ä¸€åŒ–å‘é‡ä¼˜åŒ–ï¼‰
        # å¯¹äºå•ä½å‘é‡: euclidean_distance^2 = 2 * (1 - cosine_similarity)
        eucl_dist_squared = 2 * (1 - cos_sim)
        eucl_dist = np.sqrt(np.maximum(eucl_dist_squared, 0))  # é¿å…æ•°å€¼è¯¯å·®å¯¼è‡´è´Ÿæ•°
        eucl_sim = np.exp(-eucl_dist / 2)  # ä½¿ç”¨æŒ‡æ•°è¡°å‡è½¬æ¢ä¸ºç›¸ä¼¼åº¦
        similarities['euclidean'] = eucl_sim
        
        # 4. ç‚¹ç§¯ç›¸ä¼¼åº¦ï¼ˆå¯¹å½’ä¸€åŒ–å‘é‡ç­‰åŒäºä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        dot_products = np.dot(self.command_vectors, query_vector)
        similarities['dot_product'] = dot_products
        
        # 5. è½¯ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆè€ƒè™‘ç‰¹å¾é‡è¦æ€§ï¼‰
        # å¯¹äºæ–‡æœ¬å‘é‡ï¼ŒæŸäº›ç»´åº¦å¯èƒ½æ›´é‡è¦
        feature_weights = np.abs(query_vector) + 0.1  # æŸ¥è¯¢å‘é‡çš„æƒé‡
        weighted_cos = np.sum(self.command_vectors * query_vector * feature_weights, axis=1)
        weighted_cos = weighted_cos / np.sum(feature_weights)  # å½’ä¸€åŒ–
        similarities['weighted_cosine'] = weighted_cos
        
        # 6. åŸºäºä½ç½®çš„ç›¸ä¼¼åº¦ï¼ˆè€ƒè™‘å‘é‡åˆ†é‡çš„é‡è¦æ€§å·®å¼‚ï¼‰
        # ä¸ºä¸åŒä½ç½®çš„ç‰¹å¾åˆ†é…ä¸åŒæƒé‡
        position_weights = np.linspace(1.2, 0.8, 384)  # å‰é¢çš„ç‰¹å¾æƒé‡ç¨é«˜
        position_sim = np.sum(self.command_vectors * query_vector * position_weights, axis=1)
        position_sim = position_sim / np.sum(position_weights)
        similarities['position_weighted'] = position_sim
        
        return similarities
    
    def adaptive_weighted_fusion_final(self, similarities: Dict[str, np.ndarray], 
                                     query_vector: np.ndarray) -> np.ndarray:
        """
        æœ€ç»ˆä¼˜åŒ–çš„è‡ªé€‚åº”æƒé‡èåˆ
        
        Args:
            similarities: ä¸åŒç›¸ä¼¼åº¦è®¡ç®—ç»“æœ
            query_vector: æŸ¥è¯¢å‘é‡
            
        Returns:
            èåˆåçš„ç›¸ä¼¼åº¦åˆ†æ•°
        """
        # åˆ†ææŸ¥è¯¢å‘é‡ç‰¹å¾
        query_sparsity = np.sum(np.abs(query_vector) < 0.01) / len(query_vector)
        query_max_component = np.max(np.abs(query_vector))
        query_entropy = -np.sum(np.abs(query_vector) * np.log(np.abs(query_vector) + 1e-8))
        
        # åŸºç¡€æƒé‡é…ç½®
        base_weights = {
            'cosine': 0.30,
            'angle': 0.20,
            'euclidean': 0.15,
            'dot_product': 0.15,
            'weighted_cosine': 0.10,
            'position_weighted': 0.10
        }
        
        # æ ¹æ®æŸ¥è¯¢ç‰¹å¾åŠ¨æ€è°ƒæ•´æƒé‡
        if query_sparsity > 0.5:  # ç¨€ç–æŸ¥è¯¢ï¼Œå¢åŠ ä½™å¼¦ç›¸ä¼¼åº¦æƒé‡
            base_weights['cosine'] += 0.1
            base_weights['weighted_cosine'] += 0.05
            base_weights['euclidean'] -= 0.075
            base_weights['position_weighted'] -= 0.075
        
        if query_max_component > 0.3:  # æœ‰æ˜æ˜¾ä¸»è¦ç‰¹å¾
            base_weights['position_weighted'] += 0.05
            base_weights['angle'] += 0.05
            base_weights['euclidean'] -= 0.05
            base_weights['dot_product'] -= 0.05
        
        if query_entropy > 3.0:  # ä¿¡æ¯ç†µé«˜ï¼Œç‰¹å¾åˆ†å¸ƒå‡åŒ€
            base_weights['weighted_cosine'] += 0.05
            base_weights['cosine'] -= 0.05
        
        # æƒé‡å½’ä¸€åŒ–
        total_weight = sum(base_weights.values())
        weights = {k: v/total_weight for k, v in base_weights.items()}
        
        # é«˜çº§å½’ä¸€åŒ–ï¼šä½¿ç”¨Sigmoidå‡½æ•°è¿›è¡Œå¹³æ»‘å½’ä¸€åŒ–
        normalized_sims = {}
        for method, scores in similarities.items():
            if method in weights:
                # è®¡ç®—åˆ†æ•°çš„ç»Ÿè®¡ä¿¡æ¯
                mean_score = np.mean(scores)
                std_score = np.std(scores)
                
                if std_score > 1e-8:
                    # Z-scoreæ ‡å‡†åŒ–åä½¿ç”¨sigmoid
                    z_scores = (scores - mean_score) / std_score
                    # ä½¿ç”¨sigmoidå‡½æ•°å°†åˆ†æ•°æ˜ å°„åˆ°(0,1)
                    normalized_scores = 1 / (1 + np.exp(-z_scores))
                else:
                    # å¦‚æœæ ‡å‡†å·®å¤ªå°ï¼Œä½¿ç”¨çº¿æ€§å½’ä¸€åŒ–
                    min_score, max_score = np.min(scores), np.max(scores)
                    if max_score > min_score:
                        normalized_scores = (scores - min_score) / (max_score - min_score)
                    else:
                        normalized_scores = np.ones_like(scores) * 0.5
                
                normalized_sims[method] = normalized_scores
        
        # åŠ æƒèåˆ
        fused_scores = np.zeros(len(self.commands_data))
        for method, weight in weights.items():
            if method in normalized_sims:
                fused_scores += weight * normalized_sims[method]
        
        return fused_scores
    
    def search(self, query: str, top_k: int = 10) -> List[Dict]:
        """
        æ‰§è¡Œä¼˜åŒ–æœç´¢
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            åŒ¹é…çš„æŒ‡ä»¤åˆ—è¡¨ï¼ŒæŒ‰ç›¸ä¼¼åº¦æ’åº
        """
        print(f"\nğŸ” æ­£åœ¨æœç´¢: '{query}'")
        
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        print("æ­£åœ¨ç”Ÿæˆå½’ä¸€åŒ–æŸ¥è¯¢å‘é‡...")
        query_vector = self.encode_text(query)
        
        # éªŒè¯æŸ¥è¯¢å‘é‡å½’ä¸€åŒ–
        query_norm = np.linalg.norm(query_vector)
        print(f"æŸ¥è¯¢å‘é‡æ¨¡é•¿: {query_norm:.6f}")
        
        # è®¡ç®—ä¼˜åŒ–ç›¸ä¼¼åº¦
        print("æ­£åœ¨è®¡ç®—ä¼˜åŒ–ç›¸ä¼¼åº¦æŒ‡æ ‡...")
        similarities = self.calculate_similarities_optimized(query_vector)
        
        # æœ€ç»ˆæƒé‡èåˆ
        print("æ­£åœ¨è¿›è¡Œæœ€ç»ˆè‡ªé€‚åº”èåˆ...")
        fused_scores = self.adaptive_weighted_fusion_final(similarities, query_vector)
        
        # è·å–top-kç»“æœ
        top_indices = np.argsort(fused_scores)[::-1][:top_k]
        
        # æ„å»ºç»“æœ
        results = []
        for i, idx in enumerate(top_indices):
            result = {
                'rank': i + 1,
                'score': float(fused_scores[idx]),
                'instruction': self.commands_data[idx]['æŒ‡ä»¤'],
                'description': self.commands_data[idx]['æè¿°'],
                'original_text': self.commands_data[idx]['åŸæ–‡'],
                'cosine_similarity': float(similarities['cosine'][idx]),
                'angle_similarity': float(similarities['angle'][idx]),
                'euclidean_similarity': float(similarities['euclidean'][idx]),
                'dot_product': float(similarities['dot_product'][idx]),
                'weighted_cosine': float(similarities['weighted_cosine'][idx]),
                'position_weighted': float(similarities['position_weighted'][idx])
            }
            results.append(result)
        
        return results
    
    def compare_with_original(self, query: str, top_k: int = 5):
        """æ¯”è¾ƒæœ€ç»ˆç‰ˆæœ¬ä¸åŸå§‹ç‰ˆæœ¬çš„æœç´¢ç»“æœ"""
        print(f"\nğŸ”¬ æœç´¢ç®—æ³•å¯¹æ¯”åˆ†æ (æŸ¥è¯¢: '{query}')")
        print("="*90)
        
        query_vector = self.encode_text(query)
        similarities = self.calculate_similarities_optimized(query_vector)
        
        # æ˜¾ç¤ºå„ç§å•ä¸€æŒ‡æ ‡çš„ç»“æœ
        methods_info = [
            ('cosine', 'ä½™å¼¦ç›¸ä¼¼åº¦', 'cosine'),
            ('angle', 'è§’åº¦ç›¸ä¼¼åº¦', 'angle'),
            ('euclidean', 'æ¬§æ°è·ç¦»ç›¸ä¼¼åº¦', 'euclidean'),
            ('dot_product', 'ç‚¹ç§¯ç›¸ä¼¼åº¦', 'dot_product'),
            ('weighted_cosine', 'åŠ æƒä½™å¼¦ç›¸ä¼¼åº¦', 'weighted_cosine'),
            ('position_weighted', 'ä½ç½®åŠ æƒç›¸ä¼¼åº¦', 'position_weighted')
        ]
        
        for method_key, method_name, sim_key in methods_info:
            if sim_key in similarities:
                scores = similarities[sim_key]
                top_indices = np.argsort(scores)[::-1][:top_k]
                print(f"\nğŸ“Š {method_name} å‰{top_k}ç»“æœ:")
                for i, idx in enumerate(top_indices):
                    instruction = self.commands_data[idx]['æŒ‡ä»¤']
                    score = scores[idx]
                    print(f"  {i+1}. {instruction} (å¾—åˆ†: {score:.4f})")
        
        # æœ€ç»ˆèåˆç»“æœ
        fused_scores = self.adaptive_weighted_fusion_final(similarities, query_vector)
        top_indices = np.argsort(fused_scores)[::-1][:top_k]
        print(f"\nğŸ¯ æœ€ç»ˆä¼˜åŒ–èåˆå‰{top_k}ç»“æœ:")
        for i, idx in enumerate(top_indices):
            instruction = self.commands_data[idx]['æŒ‡ä»¤']
            score = fused_scores[idx]
            print(f"  {i+1}. {instruction} (å¾—åˆ†: {score:.4f})")
        
        print("\n" + "="*90)
    
    def interactive_search(self):
        """äº¤äº’å¼æœç´¢æ¨¡å¼"""
        print("\n" + "="*80)
        print("ğŸ¯ SCPIæŒ‡ä»¤æ™ºèƒ½æœç´¢å·¥å…· - æœ€ç»ˆä¼˜åŒ–ç‰ˆæœ¬")
        print("="*80)
        print("âœ¨ ä¸“é—¨é’ˆå¯¹å½’ä¸€åŒ–å‘é‡ä¼˜åŒ–ï¼Œç¡®ä¿æœç´¢ç²¾ç¡®æ€§")
        print("è¾“å…¥æ‚¨çš„é—®é¢˜æˆ–æè¿°ï¼Œæˆ‘å°†ä¸ºæ‚¨æ‰¾åˆ°æœ€åŒ¹é…çš„SCPIæŒ‡ä»¤")
        print("")
        print("ğŸ® å¯ç”¨å‘½ä»¤:")
        print("  â€¢ è¾“å…¥é—®é¢˜ç›´æ¥æœç´¢")
        print("  â€¢ 'compare <æŸ¥è¯¢>' - æŸ¥çœ‹è¯¦ç»†å¯¹æ¯”åˆ†æ")
        print("  â€¢ 'help' - æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯")
        print("  â€¢ 'quit' æˆ– 'exit' - é€€å‡ºç¨‹åº")
        print("-"*80)
        
        while True:
            try:
                query = input("\nğŸ’¬ è¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ").strip()
                
                if query.lower() in ['quit', 'exit', 'é€€å‡º', 'q']:
                    print("æ„Ÿè°¢ä½¿ç”¨ï¼å†è§ï¼ğŸ‘‹")
                    break
                
                if query.lower() == 'help' or query == 'å¸®åŠ©':
                    self.show_help()
                    continue
                
                if query.startswith('compare ') or query.startswith('æ¯”è¾ƒ '):
                    search_query = query.split(' ', 1)[1] if len(query.split(' ', 1)) > 1 else "acquire"
                    self.compare_with_original(search_query)
                    continue
                
                if not query:
                    print("âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆçš„é—®é¢˜ï¼")
                    continue
                
                # æ‰§è¡Œæœç´¢
                results = self.search(query, top_k=10)
                
                # æ˜¾ç¤ºç»“æœ
                self.display_results(results, query)
                
            except KeyboardInterrupt:
                print("\n\nç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­ï¼Œæ­£åœ¨é€€å‡º...")
                break
            except Exception as e:
                print(f"\nâŒ æœç´¢è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
    
    def display_results(self, results: List[Dict], query: str):
        """æ˜¾ç¤ºæœç´¢ç»“æœ"""
        print(f"\nğŸ“Š æœ€ç»ˆä¼˜åŒ–æœç´¢ç»“æœ (æŸ¥è¯¢: '{query}')")
        print("="*100)
        
        for result in results:
            # ä½¿ç”¨æ¸å˜è‰²emojiè¡¨ç¤ºæ’å
            rank_emoji = ["ğŸ¥‡", "ğŸ¥ˆ", "ğŸ¥‰", "ğŸ…", "ğŸ–ï¸"][min(result['rank']-1, 4)]
            
            print(f"\n{rank_emoji} æ’å #{result['rank']} (ç»¼åˆå¾—åˆ†: {result['score']:.4f})")
            print(f"ğŸ“ æŒ‡ä»¤: {result['instruction']}")
            print(f"ğŸ“– æè¿°: {result['description']}")
            
            # æ™ºèƒ½æˆªæ–­åŸæ–‡æ˜¾ç¤º
            original = result['original_text']
            if len(original) > 150:
                # å°è¯•åœ¨å¥å·å¤„æˆªæ–­
                truncated = original[:150]
                last_period = truncated.rfind('.')
                if last_period > 100:
                    truncated = original[:last_period+1]
                else:
                    truncated = original[:150] + "..."
            else:
                truncated = original
            
            print(f"ğŸ“„ åŸæ–‡: {truncated}")
            print(f"ğŸ“ˆ è¯¦ç»†ç›¸ä¼¼åº¦æŒ‡æ ‡:")
            print(f"   â€¢ ä½™å¼¦ç›¸ä¼¼åº¦: {result['cosine_similarity']:.4f}")
            print(f"   â€¢ è§’åº¦ç›¸ä¼¼åº¦: {result['angle_similarity']:.4f}")
            print(f"   â€¢ æ¬§æ°ç›¸ä¼¼åº¦: {result['euclidean_similarity']:.4f}")
            print(f"   â€¢ ç‚¹ç§¯ç›¸ä¼¼åº¦: {result['dot_product']:.4f}")
            print(f"   â€¢ åŠ æƒä½™å¼¦: {result['weighted_cosine']:.4f}")
            print(f"   â€¢ ä½ç½®åŠ æƒ: {result['position_weighted']:.4f}")
            print("-" * 100)
    
    def show_help(self):
        """æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯"""
        print("\n" + "="*80)
        print("ğŸ“– æœ€ç»ˆä¼˜åŒ–ç‰ˆSCPIæœç´¢å·¥å…·å¸®åŠ©")
        print("="*80)
        print("ğŸ¯ è¿™æ˜¯ä¸“é—¨é’ˆå¯¹å½’ä¸€åŒ–å‘é‡ä¼˜åŒ–çš„SCPIæŒ‡ä»¤æœç´¢å·¥å…·")
        print("")
        print("ğŸ” æœç´¢ç¤ºä¾‹:")
        print("  â€¢ 'å¦‚ä½•è®¾ç½®é‡‡é›†æ¨¡å¼'")
        print("  â€¢ 'æŸ¥çœ‹é¢‘ç‡æµ‹é‡'")
        print("  â€¢ 'è§¦å‘é…ç½®'")
        print("  â€¢ 'acquire measurement'")
        print("  â€¢ 'trigger settings'")
        print("")
        print("âš™ï¸ æœ€ç»ˆä¼˜åŒ–ç‰¹æ€§:")
        print("  ğŸ§  æ™ºèƒ½å‘é‡åŒ¹é…:")
        print("    â€¢ è‡ªåŠ¨å½’ä¸€åŒ–æŸ¥è¯¢å‘é‡")
        print("    â€¢ é’ˆå¯¹å•ä½å‘é‡ä¼˜åŒ–çš„è·ç¦»è®¡ç®—")
        print("    â€¢ è€ƒè™‘å‘é‡åˆ†é‡é‡è¦æ€§å·®å¼‚")
        print("")
        print("  ğŸ“Š 6ç§ç›¸ä¼¼åº¦è®¡ç®—:")
        print("    â€¢ ä½™å¼¦ç›¸ä¼¼åº¦ - è¯­ä¹‰ç›¸ä¼¼æ€§")
        print("    â€¢ è§’åº¦ç›¸ä¼¼åº¦ - å‘é‡å¤¹è§’")
        print("    â€¢ æ¬§æ°è·ç¦»ç›¸ä¼¼åº¦ - å‡ ä½•è·ç¦»(å½’ä¸€åŒ–ä¼˜åŒ–)")
        print("    â€¢ ç‚¹ç§¯ç›¸ä¼¼åº¦ - å‘é‡æŠ•å½±")
        print("    â€¢ åŠ æƒä½™å¼¦ç›¸ä¼¼åº¦ - ç‰¹å¾é‡è¦æ€§åŠ æƒ")
        print("    â€¢ ä½ç½®åŠ æƒç›¸ä¼¼åº¦ - è€ƒè™‘ç‰¹å¾ä½ç½®")
        print("")
        print("  ğŸ›ï¸ è‡ªé€‚åº”æƒé‡:")
        print("    â€¢ æ ¹æ®æŸ¥è¯¢ç¨€ç–æ€§è°ƒæ•´")
        print("    â€¢ è€ƒè™‘ä¸»è¦ç‰¹å¾å¼ºåº¦")
        print("    â€¢ åŸºäºä¿¡æ¯ç†µåŠ¨æ€ä¼˜åŒ–")
        print("")
        print("ğŸ® ç‰¹æ®Šå‘½ä»¤:")
        print("  â€¢ 'compare <æŸ¥è¯¢>' - è¯¦ç»†å¯¹æ¯”åˆ†æ")
        print("  â€¢ 'help' - æ˜¾ç¤ºæ­¤å¸®åŠ©")
        print("  â€¢ 'quit' - é€€å‡ºç¨‹åº")
        print("-" * 80)


def main():
    parser = argparse.ArgumentParser(description='SCPIæŒ‡ä»¤æ™ºèƒ½æœç´¢å·¥å…· - æœ€ç»ˆä¼˜åŒ–ç‰ˆæœ¬')
    parser.add_argument('--json_file', '-f', default='all_scpi_commands.json',
                       help='SCPIæŒ‡ä»¤JSONæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--model', '-m', default='sentence-transformers/all-MiniLM-L6-v2',
                       help='ä½¿ç”¨çš„transformeræ¨¡å‹')
    parser.add_argument('--query', '-q', help='ç›´æ¥æ‰§è¡ŒæŸ¥è¯¢ï¼ˆéäº¤äº’æ¨¡å¼ï¼‰')
    parser.add_argument('--top_k', '-k', type=int, default=10,
                       help='è¿”å›ç»“æœæ•°é‡')
    parser.add_argument('--compare', '-c', action='store_true',
                       help='æ˜¾ç¤ºè¯¦ç»†å¯¹æ¯”åˆ†æ')
    
    args = parser.parse_args()
    
    try:
        # åˆå§‹åŒ–æœç´¢å™¨
        searcher = SCPICommandSearcherFinal(args.json_file, args.model)
        
        if args.query:
            if args.compare:
                # å¯¹æ¯”åˆ†ææ¨¡å¼
                searcher.compare_with_original(args.query, args.top_k)
            else:
                # ç›´æ¥æŸ¥è¯¢æ¨¡å¼
                results = searcher.search(args.query, args.top_k)
                searcher.display_results(results, args.query)
        else:
            # äº¤äº’æ¨¡å¼
            searcher.interactive_search()
            
    except Exception as e:
        print(f"âŒ ç¨‹åºå¯åŠ¨å¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main() 